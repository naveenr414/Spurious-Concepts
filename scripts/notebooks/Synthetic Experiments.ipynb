{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6903c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ceddfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d2abcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/njr61/rds/hpc-work/spurious-concepts/ConceptBottleneck')\n",
    "sys.path.append('/home/njr61/rds/hpc-work/spurious-concepts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "863807d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from captum.attr import visualization as viz\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import cv2\n",
    "from copy import copy \n",
    "import itertools\n",
    "import json\n",
    "import argparse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c3d48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConceptBottleneck.CUB.models import ModelXtoC, ModelOracleCtoY\n",
    "from ConceptBottleneck.CUB.dataset import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f35f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.images import *\n",
    "from src.util import *\n",
    "from src.models import *\n",
    "from src.plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de0db99",
   "metadata": {},
   "source": [
    "## Set up dataset + model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5050155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"Your script description here\")\n",
    "\n",
    "# # Add command-line arguments\n",
    "# parser.add_argument('--num_objects', type=int, default=2, help='Number of objects')\n",
    "# parser.add_argument('--noisy', action='store_true', help='Enable noisy')\n",
    "# parser.add_argument('--weight_decay', type=float, default=0.0004, help='Weight decay value')\n",
    "# parser.add_argument('--encoder_model', type=str, default='inceptionv3', help='Encoder model')\n",
    "# parser.add_argument('--optimizer', type=str, default='sgd', help='Optimizer')\n",
    "# parser.add_argument('--seed', type=int, default=42, help='Random seed')\n",
    "\n",
    "# # Parse the command-line arguments\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# # Now you can access the variables using args.num_objects, args.noisy, etc.\n",
    "# num_objects = args.num_objects\n",
    "# noisy = args.noisy\n",
    "# weight_decay = args.weight_decay\n",
    "# encoder_model = args.encoder_model\n",
    "# optimizer = args.optimizer\n",
    "# seed = args.seed\n",
    "\n",
    "num_objects = 1\n",
    "noisy=False\n",
    "weight_decay = 0.0004\n",
    "encoder_model='mlp_0'\n",
    "optimizer = 'sgd'\n",
    "seed = 42\n",
    "\n",
    "results_folder = \"results/synthetic/objects={}_noisy={}_wd={}_model={}_optimizer={}_seed={}\".format(\n",
    "    num_objects,noisy,weight_decay,encoder_model,optimizer,seed\n",
    ")\n",
    "\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "330ceefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1537748806b0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aaf2b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, train_pkl, val_pkl = get_data(num_objects, noisy,encoder_model=encoder_model)\n",
    "val_images, val_y, val_c = unroll_data(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04f381ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model = get_synthetic_model(num_objects,encoder_model,noisy,weight_decay,optimizer,seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf142098",
   "metadata": {},
   "source": [
    "## Plot the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d83949b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images_show = 5\n",
    "for i in range(num_images_show):\n",
    "    img_path = '../cem/cem/'+train_pkl[i]['img_path']\n",
    "    image = Image.open(img_path)\n",
    "    image.save(\"{}/{}.png\".format(results_folder,i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f3012",
   "metadata": {},
   "source": [
    "## Analyze Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "af8f3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, val_acc = get_accuracy(joint_model,run_joint_model,train_loader).item(), get_accuracy(joint_model,run_joint_model,val_loader).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d07037f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_by_concept_train = get_concept_accuracy_by_concept(joint_model,run_joint_model,train_loader,sigmoid=True).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d7271",
   "metadata": {},
   "source": [
    "## Analyze Concept-Task Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1966b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_weights, y_pred, c_pred = get_attribute_class_weights(\n",
    "    joint_model,run_joint_model,joint_model.sec_model.linear.weight,val_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd315d4c",
   "metadata": {},
   "source": [
    "## Analyze Concept-Input Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64318b",
   "metadata": {},
   "source": [
    "### Counterfactual Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c275ad7",
   "metadata": {},
   "source": [
    "#### Blank, Filled, and Half Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "605e5078",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_color = torch.Tensor([0.25,0.25,0.25])\n",
    "full_color = torch.Tensor([-0.25,-0.25,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0032422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8d874a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_image = no_color.clone().view(3, 1, 1).expand((3,image_size,image_size))\n",
    "full_image = full_color.clone().view(3, 1, 1).expand_as(blank_image)\n",
    "\n",
    "half_left = no_color.view(3, 1, 1).expand_as(blank_image).clone()\n",
    "half_left[:,:,:image_size//2] = full_image[:,:,:image_size//2]\n",
    "\n",
    "half_right = no_color.clone().view(3, 1, 1).expand_as(blank_image).clone()\n",
    "half_right[:,:,image_size//2:] = full_image[:,:,image_size//2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "847a825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = [blank_image,full_image,half_left,half_right]\n",
    "all_images = torch.stack(all_images)\n",
    "str_names = [\"Blank\",\"Full\",\"Half-Left\",\"Half-Right\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00b02154",
   "metadata": {},
   "outputs": [],
   "source": [
    "y,c = run_joint_model(joint_model,all_images)\n",
    "c = c.T\n",
    "c = torch.nn.Sigmoid()(c).detach().numpy()\n",
    "c = np.round(c,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ef23f",
   "metadata": {},
   "source": [
    "### Maximal Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "646a288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_pil(img):\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([2, 2, 2])\n",
    "\n",
    "    unnormalized_image = img * std[:, np.newaxis, np.newaxis] + mean[:, np.newaxis, np.newaxis]\n",
    "    unnormalized_image = unnormalized_image*255 \n",
    "    unnormalized_image = np.clip(unnormalized_image, 0, 255).astype(np.uint8) \n",
    "    im = Image.fromarray(unnormalized_image.transpose(1,2,0))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02e51a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_values = []\n",
    "\n",
    "for concept_num in range(num_objects*2):\n",
    "    ret_image = get_maximal_activation(joint_model,run_joint_model,concept_num,\n",
    "                                       get_valid_image_function(concept_num,num_objects,epsilon=32))\n",
    "    activation_values.append (\n",
    "        torch.nn.Sigmoid()(run_joint_model(joint_model,ret_image)[1])[concept_num][0].detach().numpy()\n",
    "    )\n",
    "    \n",
    "    ret_image = ret_image.detach()[0].numpy()\n",
    "    im = numpy_to_pil(ret_image) \n",
    "    im.save(\"{}/{}.png\".format(results_folder,\"adversarial_{}\".format(concept_num)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c806c26a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(1., dtype=float32), array(1., dtype=float32)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efcccf",
   "metadata": {},
   "source": [
    "### Patch-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bdd84d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 3\n",
    "concept_num = 0\n",
    "grid_width = 256//grid_size\n",
    "combinations = list(itertools.product([0, 1], repeat=grid_size**2))\n",
    "combinations_grid = [[list(combination[i:i+grid_size]) for i in range(0, grid_size**2, grid_size)] for combination in combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fac354cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combo_to_image(combo):\n",
    "    default_image = no_color.clone().view(3, 1, 1).expand((3,256,256)).clone()\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            if combo[i][j] == 1:\n",
    "                color_tensor = full_color.view(3, 1, 1).expand((3, grid_width, grid_width))\n",
    "                default_image[:,i*grid_width:(i+1)*grid_width,j*grid_width:\n",
    "                              (j+1)*grid_width] = color_tensor\n",
    "                \n",
    "                \n",
    "    return default_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "69216d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combination_to_string(combination):\n",
    "    return ''.join(str(element) for row in combination for element in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ad6b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = torch.stack([combo_to_image(i) for i in combinations_grid])\n",
    "y,c = run_joint_model(joint_model,all_images)\n",
    "c = torch.nn.Sigmoid()(c)\n",
    "c = c.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bf55d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_to_score = {}\n",
    "for i,combination in enumerate(combinations_grid):\n",
    "    condensed_string = combination_to_string(combination)\n",
    "    combination_to_score[condensed_string] = c.detach().numpy()[i,concept_num]\n",
    "    \n",
    "largest_indices = np.argsort(c.detach().numpy()[:,0])[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ee0d427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_dif = 0\n",
    "max_ind = -1\n",
    "\n",
    "for i in largest_indices:\n",
    "    combo_original = combinations_grid[i]\n",
    "    modified_combo = deepcopy(combo_original)\n",
    "    for j in range(grid_size):\n",
    "        modified_combo[j][-1] = 0\n",
    "        \n",
    "    score_original = combination_to_score[combination_to_string(combo_original)]\n",
    "    score_modified = combination_to_score[combination_to_string(modified_combo)]\n",
    "\n",
    "    if score_original-score_modified > max_dif:\n",
    "        max_dif = score_original-score_modified\n",
    "        max_ind = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8626dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo_original = combinations_grid[max_ind]\n",
    "modified_combo = deepcopy(combo_original)\n",
    "for j in range(grid_size):\n",
    "    modified_combo[j][-1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0fc0cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_original = numpy_to_pil(combo_to_image(combo_original).detach().numpy())\n",
    "im_final = numpy_to_pil(combo_to_image(modified_combo).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa168eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_original.save(\"{}/{}.png\".format(results_folder,'original_combo'))\n",
    "im_final.save(\"{}/{}.png\".format(results_folder,'modified_combo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74257333",
   "metadata": {},
   "source": [
    "### Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ffa9f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mlp' not in encoder_model:\n",
    "    for method, method_name in zip(\n",
    "        [plot_gradcam,plot_integrated_gradients,plot_saliency],\n",
    "        ['gradcam','integrated_gradients','saliency']\n",
    "    ):\n",
    "        plt.axis('off')\n",
    "        ret = method(joint_model,run_joint_model,0,val_images,0,val_pkl)\n",
    "\n",
    "        if method_name == 'integrated_gradients':\n",
    "            ret[0].savefig('{}/{}.png'.format(results_folder,method_name),bbox_inches='tight')\n",
    "        else:\n",
    "            plt.savefig('{}/{}.png'.format(results_folder,method_name),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2dcfd35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = {\n",
    "    'train_accuracy': train_acc, \n",
    "    'val_accuracy': val_acc, \n",
    "    'concept_accuracy': accuracy_by_concept_train.tolist(), \n",
    "    'adversarial_activations': np.array(activation_values).tolist(),  \n",
    "    'grid_dif': float(max_dif), \n",
    "    'num_objects': num_objects, \n",
    "    'noisy': noisy,\n",
    "    'weight_decay': weight_decay,\n",
    "    'encoder_model': encoder_model, \n",
    "    'optimizer': optimizer, \n",
    "    'seed': seed,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3dc98d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(final_data,open(\"{}/results.json\".format(results_folder),\"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
