{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6903c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d2abcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr0/home/naveenr/projects/spurious_concepts/ConceptBottleneck/')\n",
    "sys.path.append('/usr0/home/naveenr/projects/spurious_concepts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "863807d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from copy import copy \n",
    "import itertools\n",
    "import json\n",
    "import argparse \n",
    "import resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f35f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.images import *\n",
    "from src.util import *\n",
    "from src.models import *\n",
    "from src.plot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124191be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CUB.template_model import InceptionAux, FC, End2EndModel, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d6d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_per_process_memory_fraction(0.5)\n",
    "resource.setrlimit(resource.RLIMIT_AS, (30 * 1024 * 1024 * 1024, -1))\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1373f139",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e5e0802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:06<00:00, 54.4MiB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9712d2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3535, 0.3362, 0.3103]], device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the image\n",
    "image = preprocess(Image.open(\"../../../../datasets/CUB/images/100.Brown_Pelican/Brown_Pelican_0141_94533.jpg\")).unsqueeze(0).to(device)\n",
    "\n",
    "# Define your text prompts\n",
    "text = clip.tokenize([\"this is a pelican\",\"this is a bird\", \"this is a chair\"]).to(device)\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "\n",
    "# Normalize the embeddings\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de0db99",
   "metadata": {},
   "source": [
    "## Set up dataset + model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 44, 'encoder_model': 'small7', 'epochs': 50, 'num_attributes': 4, 'expand_dim_encoder': 0, 'num_middle_encoder': 0, 'debugging': False, 'dataset': 'synthetic_object/synthetic_2', 'weight_decay': 0.0004, 'lr': 0.05, 'model_type': 'joint'}\n"
     ]
    }
   ],
   "source": [
    "is_jupyter = 'ipykernel' in sys.modules\n",
    "if is_jupyter:\n",
    "    num_objects = 2\n",
    "    encoder_model='small7'\n",
    "    seed = 44\n",
    "    epochs = 50\n",
    "    expand_dim_encoder = 0\n",
    "    num_middle_encoder = 0\n",
    "    train_variation = 'none'\n",
    "    scale_factor = 1.5\n",
    "    scale_lr = 5\n",
    "    model_type = 'joint'\n",
    "    noisy = False \n",
    "    weight_decay = 0.0004\n",
    "    lr = 0.05\n",
    "else:\n",
    "    parser = argparse.ArgumentParser(description=\"Synthetic Dataset Experiments\")\n",
    "\n",
    "\n",
    "    parser.add_argument('--num_objects', type=int, default=2, help='Number of objects')\n",
    "    parser.add_argument('--encoder_model', type=str, default='inceptionv3', help='Encoder model')\n",
    "    parser.add_argument('--seed', type=int, default=42, help='Random seed')\n",
    "    parser.add_argument('--epochs', type=int, default=50, help='Number of epochs')\n",
    "    parser.add_argument('--expand_dim_encoder', type=int, default=0, help='For MLPs, size of the middle layer')\n",
    "    parser.add_argument('--num_middle_encoder', type=int, default=0, help='For MLPs, number of middle layers')\n",
    "    parser.add_argument('--train_variation', type=str, default='none', help='Either \"none\", \"loss\", or \"half\"')\n",
    "    parser.add_argument('--scale_lr', type=int, default=5, help='For the half train variation, how much to decrease LR by')\n",
    "    parser.add_argument('--scale_factor', type=float, default=1.5, help='For the loss train variation, how much to scale loss by')\n",
    "    parser.add_argument('--model_type', type=str, default='joint', help='\"joint\" or \"independent\" model')\n",
    "    parser.add_argument('--noisy', dest='noisy',default=False,action='store_true')\n",
    "    parser.add_argument('--weight_decay', default=0.0004,type=float,help=\"What weight decay to use\")\n",
    "    parser.add_argument('--lr', type=float, default=0.05, help='Learning Rate')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    num_objects = args.num_objects\n",
    "    encoder_model = args.encoder_model \n",
    "    seed = args.seed \n",
    "    epochs = args.epochs \n",
    "    expand_dim_encoder = args.expand_dim_encoder\n",
    "    num_middle_encoder = args.num_middle_encoder\n",
    "    train_variation = args.train_variation \n",
    "    scale_factor = args.scale_factor \n",
    "    scale_lr = args.scale_lr \n",
    "    model_type = args.model_type \n",
    "    noisy = args.noisy\n",
    "    weight_decay = args.weight_decay \n",
    "    lr = args.lr \n",
    "\n",
    "if noisy:\n",
    "    dataset_name = \"synthetic_object/synthetic_{}_noisy\".format(num_objects)\n",
    "else:\n",
    "    dataset_name = \"synthetic_object/synthetic_{}\".format(num_objects)\n",
    "\n",
    "parameters = {\n",
    "    'seed': seed, \n",
    "    'encoder_model': encoder_model ,\n",
    "    'epochs': epochs, \n",
    "    'num_attributes': num_objects*2,\n",
    "    'expand_dim_encoder': expand_dim_encoder, \n",
    "    'num_middle_encoder': num_middle_encoder, \n",
    "    'debugging': False,\n",
    "    'dataset': dataset_name,\n",
    "    'weight_decay': weight_decay, \n",
    "    'lr': lr, \n",
    "}\n",
    "\n",
    "if train_variation != 'none':\n",
    "    parameters['train_variation'] = train_variation \n",
    "\n",
    "    if train_variation == 'half':\n",
    "        parameters['scale_lr'] = scale_lr \n",
    "    elif train_variation == 'loss':\n",
    "        parameters['scale_factor'] = scale_factor \n",
    "\n",
    "parameters['model_type'] = model_type \n",
    "\n",
    "print(parameters)\n",
    "torch.cuda.set_per_process_memory_fraction(0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "330ceefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f71d41be4b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf2b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, train_pkl, val_pkl, test_pkl = get_data(num_objects,encoder_model=encoder_model,dataset_name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_y, test_c = unroll_data(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_folder = get_log_folder(dataset_name,parameters).split(\"/\")[-1]\n",
    "results_folder = \"../../results/synthetic/{}\".format(log_folder)\n",
    "if not os.path.exists(results_folder): \n",
    "    os.makedirs(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "703e94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model = End2EndModel(EqualReceptiveFieldN(2,7,aux_logits=True,\n",
    "                            n_attributes=4, bottleneck=True, expand_dim=4),  MLP(input_dim=4, num_classes=2, expand_dim=4), False, False, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8abab6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model = get_synthetic_model(dataset_name,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "66863db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "End2EndModel(\n",
       "  (first_model): EqualReceptiveFieldN(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (all_fc): ModuleList(\n",
       "      (0): FC(\n",
       "        (relu): ReLU()\n",
       "        (fc_new): Linear(in_features=2048, out_features=4, bias=True)\n",
       "        (fc): Linear(in_features=4, out_features=1, bias=True)\n",
       "      )\n",
       "      (1): FC(\n",
       "        (relu): ReLU()\n",
       "        (fc_new): Linear(in_features=2048, out_features=4, bias=True)\n",
       "        (fc): Linear(in_features=4, out_features=1, bias=True)\n",
       "      )\n",
       "      (2): FC(\n",
       "        (relu): ReLU()\n",
       "        (fc_new): Linear(in_features=2048, out_features=4, bias=True)\n",
       "        (fc): Linear(in_features=4, out_features=1, bias=True)\n",
       "      )\n",
       "      (3): FC(\n",
       "        (relu): ReLU()\n",
       "        (fc_new): Linear(in_features=2048, out_features=4, bias=True)\n",
       "        (fc): Linear(in_features=4, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (AuxLogits): InceptionAux(\n",
       "      (conv0): BasicConv2d(\n",
       "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv1): BasicConv2d(\n",
       "        (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (all_fc): ModuleList(\n",
       "        (0): FC(\n",
       "          (relu): ReLU()\n",
       "          (fc_new): Linear(in_features=768, out_features=4, bias=True)\n",
       "          (fc): Linear(in_features=4, out_features=1, bias=True)\n",
       "        )\n",
       "        (1): FC(\n",
       "          (relu): ReLU()\n",
       "          (fc_new): Linear(in_features=768, out_features=4, bias=True)\n",
       "          (fc): Linear(in_features=4, out_features=1, bias=True)\n",
       "        )\n",
       "        (2): FC(\n",
       "          (relu): ReLU()\n",
       "          (fc_new): Linear(in_features=768, out_features=4, bias=True)\n",
       "          (fc): Linear(in_features=4, out_features=1, bias=True)\n",
       "        )\n",
       "        (3): FC(\n",
       "          (relu): ReLU()\n",
       "          (fc_new): Linear(in_features=768, out_features=4, bias=True)\n",
       "          (fc): Linear(in_features=4, out_features=1, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (sec_model): MLP(\n",
       "    (linear): Linear(in_features=4, out_features=4, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fe4daed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualReceptiveFieldN(nn.Module):\n",
    "    def __init__(self, num_classes, num_layers,aux_logits=True, transform_input=False, \n",
    "                 n_attributes=0, bottleneck=False, expand_dim=0, \n",
    "                 three_class=False, connect_CY=False):\n",
    "        super(EqualReceptiveFieldN, self).__init__()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        channels = 512\n",
    "        for i in range(num_layers):\n",
    "            in_channels = 3 if i == 0 else 2**(i+6)\n",
    "            out_channels = 2**(i+7) \n",
    "            in_channels = min(in_channels,1024)\n",
    "            out_channels = min(out_channels,1024)\n",
    "            if i == 0:\n",
    "                conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size=15-2*(num_layers-1), stride=1, padding=1)\n",
    "            else:\n",
    "                conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv_layers.append(conv_layer)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.input_size = 256\n",
    "\n",
    "        dataset = \"synthetic\"\n",
    "\n",
    "        if n_attributes == 112:\n",
    "            dataset = \"cub\"\n",
    "        elif n_attributes == 15:\n",
    "            dataset = \"coco\"\n",
    "\n",
    "        # Calculate the output size of the last conv layer before the linear layer\n",
    "        first_num = min(512,256*2**(num_layers-3))\n",
    "        \n",
    "        if dataset == \"cub\" or dataset == \"coco\":\n",
    "            second_num = 37//(2**(num_layers-3))\n",
    "        elif dataset == \"synthetic\":\n",
    "            second_num = 32//(2**(num_layers-3))\n",
    "        else:\n",
    "            second_num = 32//(2**(num_layers-3))\n",
    "        self.conv_output_size = first_num*second_num**2\n",
    "\n",
    "        self.all_fc = nn.ModuleList()\n",
    "        \n",
    "        self.aux_logits = aux_logits\n",
    "        self.transform_input = transform_input\n",
    "        self.n_attributes = n_attributes\n",
    "        self.bottleneck = bottleneck\n",
    "        if aux_logits:\n",
    "            self.AuxLogits = InceptionAux(768, num_classes, n_attributes=self.n_attributes, bottleneck=bottleneck, \\\n",
    "                                                expand_dim=expand_dim, three_class=three_class, connect_CY=connect_CY)\n",
    "\n",
    "        if connect_CY:\n",
    "            self.cy_fc = FC(n_attributes, num_classes, expand_dim)\n",
    "        else:\n",
    "            self.cy_fc = None\n",
    "\n",
    "            \n",
    "        if self.n_attributes > 0:\n",
    "            if not bottleneck: #multitasking\n",
    "                self.all_fc.append(FC(self.conv_output_size, num_classes, expand_dim))\n",
    "            for i in range(self.n_attributes):\n",
    "                self.all_fc.append(FC(self.conv_output_size, 1, expand_dim))\n",
    "        else:\n",
    "            self.all_fc.append(FC(self.conv_output_size, num_classes, expand_dim))\n",
    "\n",
    "    def forward(self, x,binary=False):\n",
    "        for conv_layer in self.conv_layers:\n",
    "            x = self.pool(torch.relu(conv_layer(x)))        \n",
    "        self.last_conv_output = x\n",
    "\n",
    "        # Flatten the tensor before passing it through the fully connected layers\n",
    "        x = x.view(-1, self.conv_output_size)\n",
    "        self.output_before_fc = x\n",
    "\n",
    "        out = []\n",
    "        for fc in self.all_fc:\n",
    "            out.append(fc(x))\n",
    "        if self.n_attributes > 0 and not self.bottleneck and self.cy_fc is not None:\n",
    "            attr_preds = torch.cat(out[1:], dim=1)\n",
    "            if binary:\n",
    "                attr_preds = torch.round(attr_preds).float()\n",
    "            \n",
    "            out[0] += self.cy_fc(attr_preds)\n",
    "        if self.training and self.aux_logits:\n",
    "            return out, out\n",
    "        else:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0fe5fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_receptive_field(model):\n",
    "    # Initialize input size\n",
    "    input_size = (1, 3, 224, 224)  # Example input size (batch_size, channels, height, width)\n",
    "    receptive_field = 1\n",
    "    \n",
    "    # Track input size through layers\n",
    "    with torch.no_grad():\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                kernel_size = module.kernel_size[0]  # Assuming square kernels\n",
    "                stride = module.stride[0]  # Assuming square stride\n",
    "                dilation = module.dilation[0]  # Assuming square dilation\n",
    "                receptive_field += (kernel_size - 1) * dilation\n",
    "                receptive_field *= stride\n",
    "            elif isinstance(module, nn.MaxPool2d):\n",
    "                kernel_size = module.kernel_size  # Assuming square kernels\n",
    "                stride = module.stride  # Assuming square stride\n",
    "                receptive_field *= stride\n",
    "            elif isinstance(module, nn.ConvTranspose2d):\n",
    "                kernel_size = module.kernel_size[0]  # Assuming square kernels\n",
    "                stride = module.stride[0]  # Assuming square stride\n",
    "                receptive_field += (kernel_size - 1) * stride\n",
    "            elif isinstance(module, nn.Upsample):\n",
    "                scale_factor = module.scale_factor\n",
    "                receptive_field *= scale_factor\n",
    "                \n",
    "    return receptive_field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = [11,3,3]\n",
    "6+np.sum(2*(np.array(K)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e51830a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_receptive_field(EqualReceptiveFieldN(4,7,aux_logits=True,\n",
    "                            n_attributes=4, bottleneck=True, expand_dim=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c698a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cem_location = \"../../cem/results/synthetic/synthetic_2_{}.pth\".format(seed)\n",
    "# probcbm_location = \"../../prob-cbm/cem/results/synthetic/ProbCBM_fold_{}.pth\".format(seed-41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b6f8eb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joint_model = torch.load(probcbm_location,map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1e9d413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_model_function = run_probcbm_model\n",
    "run_model_function = run_joint_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ba476851",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model = joint_model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf142098",
   "metadata": {},
   "source": [
    "## Plot the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory = \"../../../../datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d83949b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f714d81bad0>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_path = dataset_directory+'/'+train_pkl[0]['img_path']\n",
    "image = Image.open(img_path)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd315d4c",
   "metadata": {},
   "source": [
    "## Analyze Concept-Input Relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9ef23f",
   "metadata": {},
   "source": [
    "### Maximal Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "646a288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_pil(img):\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([2, 2, 2])\n",
    "\n",
    "    unnormalized_image = img * std[:, np.newaxis, np.newaxis] + mean[:, np.newaxis, np.newaxis]\n",
    "    unnormalized_image = unnormalized_image*255 \n",
    "    unnormalized_image = np.clip(unnormalized_image, 0, 255).astype(np.uint8) \n",
    "    im = Image.fromarray(unnormalized_image.transpose(1,2,0))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "02e51a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On concept 0\n",
      "Output 0 [tensor([[ 0.3385, -0.0384],\n",
      "        [ 0.3385, -0.0382]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[0.2057],\n",
      "        [0.2043]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[0.1820],\n",
      "        [0.1822]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[0.2795],\n",
      "        [0.2798]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[0.1409],\n",
      "        [0.1407]], device='cuda:0', grad_fn=<AddmmBackward0>)] 1 [tensor([[ 0.3385, -0.0384],\n",
      "        [ 0.3385, -0.0382]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[0.2057],\n",
      "        [0.2043]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[0.1820],\n",
      "        [0.1822]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[0.2795],\n",
      "        [0.2798]], device='cuda:0', grad_fn=<AddmmBackward0>), tensor([[0.1409],\n",
      "        [0.1407]], device='cuda:0', grad_fn=<AddmmBackward0>)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1165414/579587685.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         ret_image = get_maximal_activation(joint_model,run_model_function,concept_num,\n\u001b[0;32m---> 14\u001b[0;31m                                         get_valid_image_function(concept_num,num_objects,epsilon=32),fixed_image=input_image,current_concept_val=current_concept_val).to(device)\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mpredicted_concept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_model_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoint_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mret_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconcept_num\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/concepts/lib/python3.7/site-packages/src/models.py\u001b[0m in \u001b[0;36mget_maximal_activation\u001b[0;34m(model, model_function, concept_num, fix_image, lamb, image_size, fixed_image, current_concept_val)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mc_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/concepts/lib/python3.7/site-packages/src/models.py\u001b[0m in \u001b[0;36mrun_joint_model\u001b[0;34m(model, x, detach)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output 0 {} 1 {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mc_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "activation_values = []\n",
    "\n",
    "for concept_num in range(num_objects*2):\n",
    "    print(\"On concept {}\".format(concept_num))\n",
    "    val_for_concept = 0\n",
    "    trials = 5\n",
    "\n",
    "    for _ in range(trials):\n",
    "        data_point = random.randint(0,len(test_images)-1)\n",
    "        input_image = deepcopy(test_images[data_point:data_point+1])\n",
    "        current_concept_val = test_c[data_point][concept_num]\n",
    "\n",
    "        ret_image = get_maximal_activation(joint_model,run_model_function,concept_num,\n",
    "                                        get_valid_image_function(concept_num,num_objects,epsilon=32),fixed_image=input_image,current_concept_val=current_concept_val).to(device)\n",
    "        predicted_concept = torch.nn.Sigmoid()(run_model_function(joint_model,ret_image)[1].detach().cpu())[concept_num][0].detach().numpy()\n",
    "        \n",
    "        val_for_concept += abs(predicted_concept-current_concept_val.detach().numpy())/trials \n",
    "        ret_image = ret_image.detach()[0].cpu().numpy()\n",
    "        im = numpy_to_pil(ret_image) \n",
    "        plt.imshow(im)\n",
    "        im.save(\"{}/{}.png\".format(results_folder,\"adversarial_{}\".format(concept_num)))\n",
    "    activation_values.append(val_for_concept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'independent':\n",
    "    joint_model[0] = joint_model[0].cpu()\n",
    "    joint_model[1] = joint_model[1].cpu() \n",
    "else: \n",
    "    joint_model = joint_model.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741d122",
   "metadata": {},
   "source": [
    "## Analyze Saliency Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dcfd35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = {\n",
    "    'adversarial_activations': np.array(activation_values).tolist(),  \n",
    "    'parameters': parameters, \n",
    "    'run_name': log_folder,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dc98d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(final_data,open(\"../../results/probcbm_results/probcbm_{}.json\".format(seed-41),\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d70f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
